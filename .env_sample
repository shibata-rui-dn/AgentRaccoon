# LLM Provider Configuration
# Choose provider: openai, azure, or gemini (default: openai)
LLM_PROVIDER=openai

# OpenAI Configuration
OPENAI_API_KEY=xxx
OPENAI_MODEL=gpt-4

# Azure OpenAI Configuration
# AZURE_OPENAI_API_KEY=xxx
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
# AZURE_OPENAI_DEPLOYMENT=your-deployment-name
# AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Google Gemini Configuration
# GEMINI_API_KEY=xxx
# GEMINI_MODEL=gemini-1.5-pro

# ポート設定
BACKEND_PORT=3002
FRONTEND_PORT=3000

# キャッシュ設定（オプション）
# パイプライン実行結果の最大行数（デフォルト: 1000）
CACHE_MAX_EXECUTION_ROWS=1000
# ノード中間結果の最大行数（デフォルト: 10000）
CACHE_MAX_NODE_ROWS=10000
# ダッシュボード要素の最大行数（デフォルト: 500）
CACHE_MAX_DASHBOARD_ROWS=500
# パイプライン実行結果のTTL（ミリ秒、デフォルト: 300000 = 5分）
CACHE_PIPELINE_TTL=300000
# ノードメタデータのTTL（ミリ秒、デフォルト: 1800000 = 30分）
CACHE_METADATA_TTL=1800000
# ダッシュボードキャッシュのTTL（ミリ秒、デフォルト: 600000 = 10分）
CACHE_DASHBOARD_TTL=600000
# キャッシュの最大エントリ数（デフォルト: 100）
CACHE_MAX_ENTRIES=100